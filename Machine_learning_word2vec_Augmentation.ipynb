{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTTUckz3DQGa"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade gensim -q\n",
        "!pip install matplotlib -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import string"
      ],
      "metadata": {
        "id": "_wcQRMf1DR0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gensim.__version__\n",
        "\n",
        "#o/p:-4.2.0"
      ],
      "metadata": {
        "id": "RCGmhWxyDbYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "y_JusVFZDcFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "SKpJ3gogDmp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "id": "FPz6YdDpD0cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wv = api.load('word2vec-google-news-300')\n",
        "type(wv)\n"
      ],
      "metadata": {
        "id": "VwyC8N2JD8_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "from nltk.stem import WordNetLemmatizer # for lemmatization\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "!pip install catboost\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split , cross_val_score,KFold\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "!pip install imblearn\n",
        "import imblearn\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "pRyPbEqmEn-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "metadata": {
        "id": "5_hDSt8JRogd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading Training Data\n",
        "nRowsRead = None\n",
        "X_train_file = pd.read_csv('Train-Augmentation.csv', delimiter=',', nrows=nRowsRead, encoding=\"ISO-8859-1\")\n",
        "nRow, nCol = X_train_file.shape\n",
        "print(f'Training Data: {nRow} rows, {nCol} columns')\n",
        "\n",
        "# Reading Test Data\n",
        "X_test_file = pd.read_csv('Test.csv', delimiter=',', nrows=nRowsRead, encoding=\"ISO-8859-1\")\n",
        "nRow, nCol = X_test_file.shape\n",
        "print(f'Test Data: {nRow} rows, {nCol} columns')"
      ],
      "metadata": {
        "id": "aa-kgfHzfczJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer and Vectorization\n",
        "def sent_vec(sent):\n",
        "    vector_size = wv.vector_size\n",
        "    wv_res = np.zeros(vector_size)\n",
        "    ctr = 1\n",
        "    for w in sent:\n",
        "        if w in wv:\n",
        "            ctr += 1\n",
        "            wv_res += wv[w]\n",
        "    return wv_res / ctr\n"
      ],
      "metadata": {
        "id": "U86fv-O7FDb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    mytokens = [\n",
        "        word.lemma_.lower().strip()\n",
        "        for word in doc\n",
        "        if word.text.lower() not in stop_words and word.text not in punctuations\n",
        "    ]\n",
        "    return mytokens\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U00iCFFuFTA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Data\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "punctuations = string.punctuation\n",
        "\n",
        "\n",
        "X_train_file['tokens'] = X_train_file['tweet'].apply(spacy_tokenizer)\n",
        "X_test_file['tokens'] = X_test_file['tweet'].apply(spacy_tokenizer)\n",
        "X_train_file['vec'] = X_train_file['tokens'].apply(sent_vec)\n",
        "X_test_file['vec'] = X_test_file['tokens'].apply(sent_vec)\n"
      ],
      "metadata": {
        "id": "jA8aatY1CK8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Training and Test Data for Model\n",
        "A = X_train_file['vec'].to_list()  # Training data features\n",
        "B = X_train_file['label']  # Training data labels\n",
        "C = X_test_file['vec'].to_list()  # Test data features\n",
        "D = X_test_file['label']  # Test data labels"
      ],
      "metadata": {
        "id": "y_lLZwHEK3b_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class Distribution in Training Data\n",
        "counter = Counter(B)\n",
        "for k, v in counter.items():\n",
        "    print(f'Class={k}, n={v} ({v / len(B) * 100:.3f}%)')\n",
        "\n",
        "plt.bar(counter.keys(), counter.values())\n",
        "plt.title('Class Distribution in Training Data')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0eO_NER5LCc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.array(X_train_file['vec'].tolist())\n",
        "y_train = np.array(X_train_file['label'].tolist())\n",
        "\n",
        "X_test = np.array(X_test_file['vec'].tolist())\n",
        "y_test = np.array(X_test_file['label'].tolist())"
      ],
      "metadata": {
        "id": "8yHZkmGsD9nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alg =[\n",
        "       LogisticRegression(random_state=42),\n",
        "       GaussianNB(),\n",
        "       DecisionTreeClassifier(random_state=42),\n",
        "       MLPClassifier(hidden_layer_sizes=(50,40), max_iter=300),\n",
        "       SGDClassifier(),\n",
        "       SVC(random_state=42),\n",
        "       RandomForestClassifier(n_estimators=300,random_state=42),\n",
        "       AdaBoostClassifier(n_estimators=300, random_state=42),\n",
        "       XGBClassifier(n_estimators=300, random_state=42),\n",
        "       LGBMClassifier(n_estimators=300, random_state=42),\n",
        "       CatBoostClassifier(n_estimators=300, random_state=42)\n",
        "       ]\n",
        "\n",
        "name = [\n",
        "        \"LogisticRegression\",\n",
        "        \"GaussianNB\",\n",
        "        \"DecisionTree\",\n",
        "        \"MLPClassifier\",\n",
        "        \"SGD\",\n",
        "        \"SVM\",\n",
        "        \"RandomForest\" ,\n",
        "        \"AdaBoost\",\n",
        "        \"XGB\",\n",
        "        \"LGBM\",\n",
        "        \"CatBoost\"\n",
        "        ]"
      ],
      "metadata": {
        "id": "YUCmrC3NPeCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# تابع نمایش ماتریس درهم‌ریختگی و محاسبه معیارهای ارزیابی\n",
        "def show_confusion(pred_label, label, nm):\n",
        "    class_names = ['Hate', 'Offensive', 'Neither']\n",
        "    cm = confusion_matrix(label, pred_label, labels=[0, 1, 2])  # Specify labels to ensure order\n",
        "\n",
        "    # نمایش ماتریس درهم‌ریختگی\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "    disp.plot()\n",
        "    plt.title(f'Confusion Matrix {nm}')\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "# تابع ارزیابی مدل\n",
        "def evaluate_model(model, X_data, y_data, nm):\n",
        "    # پیش‌بینی برای داده‌های ورودی\n",
        "    predictions = model.predict(X_data)\n",
        "\n",
        "    # محاسبه معیارهای کلی\n",
        "    precision = precision_score(y_data, predictions, average='macro')\n",
        "    recall = recall_score(y_data, predictions, average='macro')\n",
        "    accuracy = accuracy_score(y_data, predictions)\n",
        "    f1 = f1_score(y_data, predictions, average='macro')\n",
        "\n",
        "    # چاپ معیارها\n",
        "    print(f\"===== {nm} =====\")\n",
        "    print(f\"Precision: {precision:.8f}\")\n",
        "    print(f\"Recall: {recall:.8f}\")\n",
        "    print(f\"Accuracy: {accuracy:.8f}\")\n",
        "    print(f\"F1 Score: {f1:.8f}\")\n",
        "\n",
        "    # نمایش ماتریس درهم‌ریختگی\n",
        "    show_confusion(y_data, predictions, nm)\n",
        "\n",
        "    # چاپ گزارش کامل\n",
        "    print(classification_report(y_data, predictions, target_names=['Hate', 'Offensive', 'Neither'], digits=4))\n",
        "\n",
        "    # بازگرداندن معیارها برای استفاده در DataFrame\n",
        "    return precision, recall, accuracy, f1\n",
        "\n",
        "# ایجاد یک DataFrame خالی برای ذخیره نتایج\n",
        "results_df = pd.DataFrame(columns=['Model', 'Precision', 'Recall', 'F1 Score', 'Accuracy'])\n",
        "\n",
        "# برای هر مدل اجرا کنید\n",
        "for model, model_name in zip(alg, name):\n",
        "    print(f'----------------- {model_name} ----------------------')\n",
        "\n",
        "    # آموزش مدل با داده‌های آموزش\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # ارزیابی روی داده‌های آموزش\n",
        "    train_precision, train_recall, train_accuracy, train_f1 = evaluate_model(model, X_train, y_train, 'Train ' + model_name)\n",
        "\n",
        "    # ارزیابی روی داده‌های تست\n",
        "    test_precision, test_recall, test_accuracy, test_f1 = evaluate_model(model, X_test, y_test, 'Test ' + model_name)\n",
        "\n",
        "    # اضافه کردن اطلاعات به DataFrame\n",
        "    new_row = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Precision': [test_precision],\n",
        "        'Recall': [test_recall],\n",
        "        'F1 Score': [test_f1],\n",
        "        'Accuracy': [test_accuracy]\n",
        "    })\n",
        "\n",
        "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
        "\n",
        "# نمایش جدول نتایج\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "S_u14wz8Q-m6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}